{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Traditional Classifiers with the data obtained after using CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we load all the data obtained by a fully connected layer, after training the CNN. We receive for each image 1024 values.\n",
    "\n",
    "For knowing the best values to apply to the classifiers we did GridSearch. There is the KNN, SVC, SVC Linear (which takes a more reasonable time than SVC RBF), Gradient Boosting, Random Forest and Adaboost.\n",
    "\n",
    "We can not see all the experiments in the notebook. There are too many. But we can see the final and best results in the confusions matrices saved in out/confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If you want to run the code using GPU\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import keras\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.32\n",
    "set_session(tf.Session(config=config))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to run on CPU, execute this lines before import keras and tensorflow\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Imports if you want to compute the data (than you have to use\n",
    "#keras and tensorflow). If you want to run the code with loaded data\n",
    "#there is no need to run this\n",
    "from cnn.inceptionV3.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn import grid_search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our scripts\n",
    "SCRIPTS_PATH = '/../../scripts'\n",
    "sys.path.insert(0, os.getcwd()+SCRIPTS_PATH)\n",
    "#print sys.path\n",
    "from dataset import Dataset\n",
    "\n",
    "# set the log\n",
    "logging.basicConfig(level=logging.DEBUG, format='[%(asctime)s] %(message)s',\n",
    "                    datefmt='%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "\n",
    "def preprocess_input(x):\n",
    "    x = x/255.\n",
    "    x = x - 0.5\n",
    "    x = x*2.\n",
    "    return x\n",
    "\n",
    "#Freeze the n selected layers, and work with the other layers           \n",
    "def freeze_n_first_layers(n_not_train, model, optimizer=SGD(lr=0.0001, momentum=0.9),\n",
    "                          loss='sparse_categorical_crossentropy'):\n",
    "\n",
    "    for layer in model.layers[:n_not_train]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[n_not_train:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    # we need to recompile the model for these modifications to take effect\n",
    "    # we use SGD with a low learning rate\n",
    "    model.compile(optimizer, loss)\n",
    "\n",
    "#Build the model of the CNN\n",
    "def buildModel(load_weights = False):\n",
    "    # create the base pre-trained model\n",
    "    if load_weights == False:\n",
    "        base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "    else:\n",
    "        base_model = InceptionV3(weights=None, include_top=False)\n",
    "\n",
    "    # add a global spatial average pooling layer\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    # let's add a fully-connected layer\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    # and a logistic layer with 2 classes food/no_food\n",
    "    predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "    # this is the model we will train\n",
    "    print 'Loading model...\\n'\n",
    "    model = Model(input=base_model.input, output=predictions)\n",
    "\n",
    "    if load_weights:\n",
    "        model.load_weights(WEIGHTS_PATH)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_interlayer(model, val_ds):\n",
    "    #We want to obtain the intermediate layer, to apply Classifiers to them. In layer_output we append the layer from all the batches\n",
    "    #to obtain the intermediate layer from the given dataset and not just from a batch\n",
    "    layer_output = []\n",
    "    #In label we obtain the y_true from all the given dataset\n",
    "    label = []\n",
    "    # Obtain the output of the intermediate layers \n",
    "    inter_layer_model = K.function([model.layers[0].input, K.learning_phase()], [model.layers[-2].output])\n",
    "    \n",
    "    num_validations = val_ds.num_batches()\n",
    "\n",
    "    for iter_val in range(num_validations):\n",
    "        print 'iter_val: ', iter_val+1, 'out of: ', val_ds.num_batches()\n",
    "        # Get new batch and preprocess the image according to the model\n",
    "        X, y = val_ds.get_Xy(SIZE)\n",
    "        X = preprocess_input(X)\n",
    "        \n",
    "        layer_output.append(inter_layer_model([X, 0])[0])\n",
    "        label+=y\n",
    "    return layer_output, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save the confusion matrix, with a lot of information in it (Accuracy, Accuracy normalized, Precision, Recall, F1)\n",
    "def save_confusion_matrix(y_true, y_pred, name_to_save, title='Confusion matrix ', info=''):\n",
    "\n",
    "    # create the confusion matrix and normalize it\n",
    "    cm = 1.0*confusion_matrix(y_true, y_pred, labels=range(len(set(y_true))))\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    inf = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    print '\\tPrecision:', inf[0],'\\t Recall:', inf[1]\n",
    "    print \"\\t\" +'F1 score:', inf[2]\n",
    "    # prepare the plot\n",
    "    fig = plt.figure()\n",
    "    plt.matshow(cm_norm)\n",
    "    tick_marks = np.arange(cm_norm.shape[0])\n",
    "    plt.xticks(tick_marks, rotation=45)\n",
    "    plt.yticks(tick_marks)\n",
    "    if (MULTITASK):\n",
    "        for i, j in itertools.product(range(cm_norm.shape[0]), range(cm_norm.shape[1])):\n",
    "            \"\"\"\n",
    "            plt.text(j, i, str(int(cm[i,j]))+' / '+str(int(sum(cm[i]))),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm_norm[i, j] < 0.5 else \"black\")\n",
    "            \"\"\"\n",
    "            plt.text(j, i, str(round(cm_norm[i,j]*100, 2))+'%',\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm_norm[i, j] < 0.5 else \"black\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label \\n '+info+'\\n F1 score: '+str(round(inf[2],2))+'\\n Precision: '+str(round(inf[0], 2))+', Recall: '+str(round(inf[1], 2)))\n",
    "\n",
    "    # save the figure and close so it never gets displayed\n",
    "    plt.savefig(CONF_MATRIX+name_to_save+'.png')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating or loading the data\n",
    "def get_data_model(load=False):\n",
    "    if (load):\n",
    "        #Load files\n",
    "        X_train = np.load('npdata/X_train'+TITLE+'.npy')\n",
    "        y_train = np.load('npdata/y_train'+TITLE+'.npy')\n",
    "        X_val = np.load('npdata/X_val'+TITLE+'.npy')\n",
    "        y_val = np.load('npdata/y_val'+TITLE+'.npy')\n",
    "        X_test = np.load('npdata/X_test'+TITLE+'.npy')\n",
    "        y_test = np.load('npdata/y_test'+TITLE+'.npy')\n",
    "    else:\n",
    "        # ------------------- Data\n",
    "        # prepare the training and validation dataset\n",
    "        train_ds = Dataset(TRAIN_TXT, NUM_TRAIN_IMG, BATCH_SIZE, CLASSES, \n",
    "                           data_augmentation=False, multitask = MULTITASK)\n",
    "        val_ds = Dataset(VAL_TXT, NUM_VAL_IMG, BATCH_SIZE, CLASSES, \n",
    "                         data_augmentation=False, multitask = MULTITASK)\n",
    "        test_ds = Dataset(TEST_TXT, NUM_TEST_IMG, BATCH_SIZE, CLASSES, \n",
    "                         data_augmentation=False, multitask = MULTITASK)\n",
    "\n",
    "        # ------------------- Model\n",
    "        # load the desired model and visualize it\n",
    "        model = buildModel(load_weights=True)\n",
    "\n",
    "        # ------------------- Train more Layers\n",
    "        # we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "        # the first 172 layers and unfreeze the rest:\n",
    "        if (MULTITASK):\n",
    "            freeze_n_first_layers(-1, model,loss = 'binary_crossentropy')\n",
    "        else:\n",
    "            freeze_n_first_layers(172, model, SGD(lr=0.0001, momentum=0.9))\n",
    "        \n",
    "        #Training\n",
    "        X_train, y_train = get_data_interlayer(model, train_ds)\n",
    "        X_train = np.concatenate(X_train)\n",
    "        y_train = np.asarray(y_train)\n",
    "\n",
    "        np.save('npdata/X_train'+TITLE, X_train)\n",
    "        np.save('npdata/y_train'+TITLE, y_train)\n",
    "        \n",
    "        #Validation\n",
    "        X_val, y_val = get_data_interlayer(model, val_ds)\n",
    "        X_val = np.concatenate(X_val)\n",
    "        y_val = np.asarray(y_val)\n",
    "        \n",
    "        np.save('npdata/X_val'+TITLE, X_val)\n",
    "        np.save('npdata/y_val'+TITLE, y_val)\n",
    "        \n",
    "        #Test\n",
    "        X_test, y_test = get_data_interlayer(model, test_ds)\n",
    "        X_test = np.concatenate(X_test)\n",
    "        y_test = np.asarray(y_test)\n",
    "        \n",
    "        ## Save test dataset\n",
    "        np.save('npdata/X_test'+TITLE, X_test)\n",
    "        np.save('npdata/y_test'+TITLE, y_test)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    #return X_val, X_test, y_val, y_test\n",
    "\n",
    "#Select which classifier to use. \n",
    "def accuracy_classifiers(X_train, X_test, y_train, y_test, not_grid_search=False,\n",
    "                         adaboost=False, gradient_boosting=False, random_forest=False,\n",
    "                         svc=False, linear_svc=False, k_neighbors=False, name_task='', class_weight=''):  \n",
    "        if (adaboost):\n",
    "            Adaboost_GridSearch(X_train, X_test, y_train, y_test, name_task=name_task)\n",
    "        if (gradient_boosting):\n",
    "            GradientBoosting_GridSearch(X_train, X_test, y_train, y_test, name_task=name_task)\n",
    "        if(random_forest):\n",
    "            RandomForest_GridSearch(X_train, X_test, y_train, y_test, name_task=name_task)\n",
    "        if(svc):\n",
    "            SVC_GridSearch(X_train, X_test, y_train, y_test, name_task=name_task, class_weight=class_weight)\n",
    "        if(linear_svc):\n",
    "            LinearSVC_GridSearch(X_train, X_test, y_train, y_test, name_task=name_task, class_weight=class_weight)\n",
    "        if(k_neighbors):\n",
    "            KNeighbors_GridSearch(X_train, X_test, y_train, y_test, name_task=name_task)\n",
    "            \n",
    "        if (not_grid_search):\n",
    "            otherClassifiers(X_train, X_test, y_train, y_test, name_task=name_task)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Computing accuracy with different classifiers. Without GridSearch run some classifiers, with default parameters.\n",
    "#Save a an image for each classifier with the confusion matrix and information\n",
    "def otherClassifiers(X_train, X_test, y_train, y_test, grid_search=False, name_task=''):\n",
    "    names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \n",
    "         \"Decision Tree\", \"Random Forest\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"Gradient Boosting\"]\n",
    "    classifiers = [\n",
    "        KNeighborsClassifier(3),\n",
    "        SVC(kernel=\"linear\", C=0.025),\n",
    "        #SVC(gamma=2, C=1),\n",
    "        DecisionTreeClassifier(),\n",
    "        RandomForestClassifier(n_estimators=10, max_features=1),\n",
    "        #AdaBoostClassifier(),\n",
    "        #GaussianNB(),\n",
    "        GradientBoostingClassifier()]\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    for name, clf in zip(names, classifiers):    \n",
    "        # Standard parameter\n",
    "        clf.fit(X_train, y_train)\n",
    "        yhat = clf.predict(X_test)\n",
    "        # Recall, f1, precision\n",
    "        acc = metrics.accuracy_score(yhat, y_test)\n",
    "        acc_norm = normalized_accuracy(y_test, yhat)\n",
    "        print '\\n \\n',name\n",
    "        print '\\tAccuracy:',acc, '\\t Accuracy normalized:', acc_norm\n",
    "        save_confusion_matrix(y_test, yhat, TITLE+'_'+name+'_'+name_task+'_'+TITLE+'_noGrid_'+str(round(acc,5)), \n",
    "                          title=str(name)+' '+name_task, \n",
    "                          info='Acc.: '+ str(round(acc*100,2))+',  Acc. norm.:'+str(round(acc_norm*100,2)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Computes de normalized accuracy. Given the true label and the y predicted, returns the normalized accuracy\n",
    "def normalized_accuracy(y_true, y_pred):\n",
    "    cm = 1.0*confusion_matrix(y_true, y_pred, labels=range(len(set(y_true))))\n",
    "    acc = []\n",
    "    for i, row in enumerate(cm):\n",
    "        acc.append(row[i]/sum(row))\n",
    "    return sum(acc)/len(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Given a binary matrix with labels (ex. [[1,0,0], [0, 0, 1]]) returns correspondent vector no binary ([0,2])\n",
    "def to_one_vector(y, pos):\n",
    "    return np.asarray([lbl[pos[0]:pos[1]].argmax() for lbl in y])    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adaboost Classifier applying GridSearchCV. In param_grid, you can choose the parameters to do in GridSearch\n",
    "def Adaboost_GridSearch(X, X_test, y, y_test, name_task):\n",
    "    name = 'AdaBoost'\n",
    "    print  name\n",
    "    param_grid = {\"algorithm\" : [\"SAMME.R\",\"SAMME\"], \"n_estimators\" : [1, 10, 50, 100, 200, 500]}\n",
    "    # KFold\n",
    "    nfolds = 5\n",
    "    kf = cross_validation.KFold(n=X.shape[0], n_folds=nfolds, shuffle=True, random_state=0)\n",
    "\n",
    "    yhat = np.empty_like(y)\n",
    "    acc = np.empty(nfolds)\n",
    "    acc_norm = np.empty(nfolds)\n",
    "    i = 0\n",
    "\n",
    "    best_acc = 0\n",
    "\n",
    "    for train_index, test_index in kf:\n",
    "        X_train, X_val = X[train_index], X[test_index]\n",
    "        y_train, y_val = y[train_index], y[test_index]\n",
    "\n",
    "        print \"Iteration KFold:\", i+1,'/',nfolds\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        # Standard parameters\n",
    "        clf = ensemble.AdaBoostClassifier()\n",
    "\n",
    "        # We can change the scoring \"average_precision\", \"recall\", \"f1\"\n",
    "        clf = grid_search.GridSearchCV(clf, param_grid, scoring='f1_weighted', n_jobs=4, verbose=1)\n",
    "        clf.fit(X_train,y_train.ravel())\n",
    "\n",
    "        X_val = scaler.transform(X_val)\n",
    "        yhat[test_index] = clf.predict(X_val)\n",
    "\n",
    "        acc[i] = metrics.accuracy_score(yhat[test_index], y_val)\n",
    "        acc_norm[i] = normalized_accuracy(y_val, yhat[test_index])\n",
    "        #We save the parameters with the ones we obtain better accuract\n",
    "        if (acc_norm[i]>best_acc):\n",
    "            best_acc = acc_norm[i]\n",
    "            best_param = clf.best_params_\n",
    "            \n",
    "            \n",
    "        print \"\\t\" + str(clf.best_params_), acc[i]\n",
    "        print\n",
    "        i=i+1\n",
    "\n",
    "    print 'Mean accuracy: '+ str(np.mean(acc))\n",
    "    \n",
    "    # Once we have the best parameters for the training dataset, we obtain the accuraacy for the test dataset with the best parameters obtained. \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "    clf2 = svm.LinearSVC(loss=best_param['loss'], C=best_param['C'])\n",
    "\n",
    "    clf2.fit(X_train, y)\n",
    "    yhat = clf2.predict(X_test)\n",
    "\n",
    "    acc = metrics.accuracy_score(yhat, y_test)\n",
    "    acc_norm = normalized_accuracy(y_test, yhat)\n",
    "    print best_param\n",
    "    print '\\tAccuracy:',acc, '\\t Accuracy normalized:', acc_norm\n",
    "    save_confusion_matrix(y_test, yhat, TITLE+'_'+name+'_'+name_task+'_'+TITLE+'_'+class_weight+'_'+str(round(acc,5)), \n",
    "                      title=str(name)+'_'+name_task, \n",
    "                      info='Acc.: '+ str(round(acc*100,2))+',  Acc. norm.:'+str(round(acc_norm*100,2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting applying GridSearchCV. In param_grid, you can choose the parameters to do in GridSearch\n",
    "def GradientBoosting_GridSearch(X, X_test, y, y_test, name_task):\n",
    "    name = 'GradientBoosting'\n",
    "    print  name\n",
    "    param_grid = {'n_estimators':[50, 100], 'learning_rate': [0.1, 0.01],\n",
    "                    #'max_depth': [3, 4, 6],\n",
    "                    'min_samples_leaf': [1, 3, 5]\n",
    "                    # 'max_features': [1.0, 0.3, 0.1]\n",
    "                    }\n",
    "\n",
    "    # KFold\n",
    "    nfolds = 3\n",
    "    kf = cross_validation.KFold(n=X.shape[0], n_folds=nfolds, shuffle=True, random_state=0)\n",
    "\n",
    "    yhat = np.empty_like(y)\n",
    "    acc = np.empty(nfolds)\n",
    "    acc_norm = np.empty(nfolds)\n",
    "    i = 0\n",
    "\n",
    "    best_acc = 0\n",
    "\n",
    "    for train_index, test_index in kf:\n",
    "        X_train, X_val = X[train_index], X[test_index]\n",
    "        y_train, y_val = y[train_index], y[test_index]\n",
    "\n",
    "        print \"Iteration KFold:\", i+1,'/',nfolds\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        # Standard parameters\n",
    "        clf = ensemble.GradientBoostingClassifier()\n",
    "\n",
    "        # We can change the scoring \"average_precision\", \"recall\", \"f1\"\n",
    "        clf = grid_search.GridSearchCV(clf, param_grid, scoring='f1_weighted', n_jobs=4, verbose=1)\n",
    "        clf.fit(X_train,y_train.ravel())\n",
    "\n",
    "        X_val = scaler.transform(X_val)\n",
    "        yhat[test_index] = clf.predict(X_val)\n",
    "\n",
    "        acc[i] = metrics.accuracy_score(yhat[test_index], y_val)\n",
    "        acc_norm[i] = normalized_accuracy(y_val, yhat[test_index])\n",
    "        #We save the parameters with the ones we obtain better accuract\n",
    "        if (acc_norm[i]>best_acc):\n",
    "            best_acc = acc_norm[i]\n",
    "            best_param = clf.best_params_\n",
    "            \n",
    "        print \"\\t\" + str(clf.best_params_), acc[i]\n",
    "        print\n",
    "        i=i+1\n",
    "\n",
    "    print 'Mean accuracy: '+ str(np.mean(acc))\n",
    "\n",
    "    \n",
    "    #If you want to directly do grid search and find the best parameters, and with those predict and see the accuracy of the test\n",
    "    #dataset, run also this\n",
    "    # Once we have the best parameters for the training dataset, we obtain the accuraacy for the test dataset with the best parameters obtained. \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "    clf2 = ensemble.GradientBoostingClassifier(n_estimators=best_param['n_estimators'], learning_rate=best_param['learning_rate'], \n",
    "                                              min_samples_leaf=best_param['min_samples_leaf'])\n",
    "       \n",
    "    clf2.fit(X_train, y)\n",
    "    yhat = clf2.predict(X_test)\n",
    "\n",
    "    acc = metrics.accuracy_score(yhat, y_test)\n",
    "    acc_norm = normalized_accuracy(y_test, yhat)\n",
    "    print best_param\n",
    "    print '\\tAccuracy:',acc, '\\t Accuracy normalized:', acc_norm\n",
    "    save_confusion_matrix(y_test, yhat, TITLE+'_'+name+'_'+name_task+'_'+TITLE+'_'+'_'+str(round(acc,5)), \n",
    "                      title=str(name)+'_'+name_task, \n",
    "                      info='Acc.: '+ str(round(acc*100,2))+',  Acc. norm.:'+str(round(acc_norm*100,2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random Forest applying GridSearchCV. In param_grid, you can choose the parameters to do in GridSearch\n",
    "def RandomForest_GridSearch(X, X_test, y, y_test, name_task):\n",
    "    name = 'RandomForest'\n",
    "    print  name\n",
    "    param_grid = { \n",
    "        'n_estimators': [10, 50, 100, 200, 300, 500],\n",
    "        'criterion':['gini', 'entropy'],\n",
    "        'max_features': ['auto', 'sqrt', 'log2']\n",
    "    }\n",
    "   \n",
    "    # KFold\n",
    "    nfolds = 5\n",
    "    kf = cross_validation.KFold(n=X.shape[0], n_folds=nfolds, shuffle=True, random_state=0)\n",
    "\n",
    "    yhat = np.empty_like(y)\n",
    "    acc = np.empty(nfolds)\n",
    "    acc_norm = np.empty(nfolds)\n",
    "    i = 0\n",
    "\n",
    "    best_acc = 0\n",
    "\n",
    "    for train_index, test_index in kf:\n",
    "        X_train, X_val = X[train_index], X[test_index]\n",
    "        y_train, y_val = y[train_index], y[test_index]\n",
    "\n",
    "        print \"Iteration KFold:\", i+1,'/',nfolds\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        # Standard parameters\n",
    "        clf = ensemble.RandomForestClassifier()\n",
    "\n",
    "        # We can change the scoring \"average_precision\", \"recall\", \"f1\"\n",
    "        clf = grid_search.GridSearchCV(clf, param_grid, scoring='f1_weighted', n_jobs=4, verbose=1)\n",
    "        clf.fit(X_train,y_train.ravel())\n",
    "\n",
    "        X_val = scaler.transform(X_val)\n",
    "        yhat[test_index] = clf.predict(X_val)\n",
    "\n",
    "        acc[i] = metrics.accuracy_score(yhat[test_index], y_val)\n",
    "        acc_norm[i] = normalized_accuracy(y_val, yhat[test_index])\n",
    "        #We save the parameters with the ones we obtain better accuract\n",
    "        if (acc_norm[i]>best_acc):\n",
    "            best_acc = acc_norm[i]\n",
    "            best_param = clf.best_params_\n",
    "            \n",
    "        print \"\\t\" + str(clf.best_params_), acc[i]\n",
    "        print\n",
    "        i=i+1\n",
    "\n",
    "    print 'Mean accuracy: '+ str(np.mean(acc))\n",
    "    \n",
    "    # Once we have the best parameters for the training dataset, we obtain the accuraacy for the test dataset with the best parameters obtained. \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "    clf2 = svm.LinearSVC(loss=best_param['loss'], C=best_param['C'])\n",
    "\n",
    "    clf2.fit(X_train, y)\n",
    "    yhat = clf2.predict(X_test)\n",
    "\n",
    "    acc = metrics.accuracy_score(yhat, y_test)\n",
    "    acc_norm = normalized_accuracy(y_test, yhat)\n",
    "    print best_param\n",
    "    print '\\tAccuracy:',acc, '\\t Accuracy normalized:', acc_norm\n",
    "    save_confusion_matrix(y_test, yhat, TITLE+'_'+name+'_'+name_task+'_'+TITLE+'_'+class_weight+'_'+str(round(acc,5)), \n",
    "                      title=str(name)+'_'+name_task, \n",
    "                      info='Acc.: '+ str(round(acc*100,2))+',  Acc. norm.:'+str(round(acc_norm*100,2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SVC applying GridSearchCV. In param_grid, you can choose the parameters to do in GridSearch\n",
    "def SVC_GridSearch(X, X_test, y, y_test, name_task, class_weight=''):\n",
    "    name = 'SVC'\n",
    "    print  name\n",
    "    param_grid = [{'kernel': ['rbf'], 'gamma': [0.1, 0.001, 0.0001, 'auto'], 'C': [0.01, 0.025, 0.5, 1, 10, 100], 'class_weight':[class_weight]}]\n",
    "    # KFold\n",
    "    nfolds = 5\n",
    "    kf = cross_validation.KFold(n=X.shape[0], n_folds=nfolds, shuffle=True, random_state=0)\n",
    "\n",
    "    yhat = np.empty_like(y)\n",
    "    acc = np.empty(nfolds)\n",
    "    acc_norm = np.empty(nfolds)\n",
    "    i = 0\n",
    "\n",
    "    best_acc = 0\n",
    "\n",
    "    for train_index, test_index in kf:\n",
    "        X_train, X_val = X[train_index], X[test_index]\n",
    "        y_train, y_val = y[train_index], y[test_index]\n",
    "\n",
    "        print \"Iteration KFold:\", i+1,'/',nfolds\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        # Standard parameters\n",
    "        clf = SVC()\n",
    "\n",
    "        # We can change the scoring \"average_precision\", \"recall\", \"f1\"\n",
    "        clf = grid_search.GridSearchCV(clf, param_grid, scoring='f1_weighted', n_jobs=4, verbose=1)\n",
    "        clf.fit(X_train,y_train.ravel())\n",
    "\n",
    "        X_val = scaler.transform(X_val)\n",
    "        yhat[test_index] = clf.predict(X_val)\n",
    "\n",
    "        acc[i] = metrics.accuracy_score(yhat[test_index], y_val)\n",
    "        acc_norm[i] = normalized_accuracy(y_val, yhat[test_index])\n",
    "        #We save the parameters with the ones we obtain better accuract\n",
    "        if (acc_norm[i]>best_acc):\n",
    "            best_acc = acc_norm[i]\n",
    "            best_param = clf.best_params_\n",
    "            \n",
    "        print \"\\t\" + str(clf.best_params_), acc[i]\n",
    "        print\n",
    "        i=i+1\n",
    "\n",
    "    print 'Mean accuracy: '+ str(np.mean(acc))\n",
    "    \n",
    "    # Once we have the best parameters for the training dataset, we obtain the accuraacy for the test dataset with the best parameters obtained. \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "    #if (best_param['kernel']=='rbf'):\n",
    "    clf2 = SVC(kernel=best_param['kernel'], gamma=best_param['gamma'], \n",
    "                                                   C=best_param['C'], class_weight=best_param['class_weight'])\n",
    "    #else:\n",
    "    #    clf2 = SVC(kernel=best_param['kernel'], C=best_param['C'])    \n",
    "        \n",
    "    clf2.fit(X_train, y)\n",
    "    yhat = clf2.predict(X_test)\n",
    "\n",
    "    acc = metrics.accuracy_score(yhat, y_test)\n",
    "    acc_norm = normalized_accuracy(y_test, yhat)\n",
    "    print best_param\n",
    "    print '\\tAccuracy:',acc, '\\t Accuracy normalized:', acc_norm\n",
    "    save_confusion_matrix(y_test, yhat, TITLE+'_'+name+'_'+name_task+'_'+TITLE+'_'+class_weight+'_'+str(round(acc,5)), \n",
    "                      title=str(name)+'_'+name_task, \n",
    "                      info='Acc.: '+ str(round(acc*100,2))+',  Acc. norm.:'+str(round(acc_norm*100,2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear SVC applying GridSearchCV. In param_grid, you can choose the parameters to do in GridSearch\n",
    "def LinearSVC_GridSearch(X, X_test, y, y_test, name_task, class_weight):\n",
    "    name = 'LinearSVC'\n",
    "    print  name\n",
    "    param_grid = [{'C': [0.001, 0.01, 0.1, 0.5, 1, 10, 100], 'loss': ['hinge', 'squared_hinge'], 'class_weight':[class_weight]}]\n",
    "                 #{'C': [0.001, 0.01, 0.1, 0.5, 1, 10, 100, 1000], 'loss': ['hinge', 'squared_hinge'], 'class_weight':{0:1.0, 1:5582/96. ,2: 5582/138. , 3:1., 4:5582/1409,5: 1.0,6:5582/4173 }}]\n",
    "    print class_weight\n",
    "    # KFold\n",
    "    nfolds = 3\n",
    "    kf = cross_validation.KFold(n=X.shape[0], n_folds=nfolds, shuffle=True, random_state=0)\n",
    "\n",
    "    yhat = np.empty_like(y)\n",
    "    acc = np.empty(nfolds)\n",
    "    acc_norm = np.empty(nfolds)\n",
    "    i = 0\n",
    "\n",
    "    best_acc = 0\n",
    "\n",
    "    for train_index, test_index in kf:\n",
    "        X_train, X_val = X[train_index], X[test_index]\n",
    "        y_train, y_val = y[train_index], y[test_index]\n",
    "\n",
    "        print \"Iteration KFold:\", i+1,'/',nfolds\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        # Standard parameters\n",
    "        clf = svm.LinearSVC()\n",
    "\n",
    "        # We can change the scoring \"average_precision\", \"recall\", \"f1\"\n",
    "        clf = grid_search.GridSearchCV(clf, param_grid, scoring='f1_weighted', n_jobs=3, verbose=1)\n",
    "        clf.fit(X_train,y_train.ravel())\n",
    "\n",
    "        X_val = scaler.transform(X_val)\n",
    "        yhat[test_index] = clf.predict(X_val)\n",
    "\n",
    "        acc[i] = metrics.accuracy_score(yhat[test_index], y_val)\n",
    "        acc_norm[i] = normalized_accuracy(y_val, yhat[test_index])\n",
    "        #We save the parameters with the ones we obtain better accuract\n",
    "        if (acc_norm[i]>best_acc):\n",
    "            best_acc = acc_norm[i]\n",
    "            best_param = clf.best_params_\n",
    "            \n",
    "        print \"\\t\" + str(clf.best_params_), acc[i]\n",
    "        print\n",
    "        i=i+1\n",
    "\n",
    "    print 'Mean accuracy: '+ str(np.mean(acc))\n",
    "\n",
    "    #If you want to directly do grid search and find the best parameters, and with those predict and see the accuracy of the test\n",
    "    #dataset, run also this\n",
    "    # Once we have the best parameters for the training dataset, we obtain the accuraacy for the test dataset with the best parameters obtained. \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "    clf2 = svm.LinearSVC(loss=best_param['loss'], C=best_param['C'])\n",
    "\n",
    "    clf2.fit(X_train, y)\n",
    "    yhat = clf2.predict(X_test)\n",
    "\n",
    "    acc = metrics.accuracy_score(yhat, y_test)\n",
    "    acc_norm = normalized_accuracy(y_test, yhat)\n",
    "    print best_param\n",
    "    print '\\tAccuracy:',acc, '\\t Accuracy normalized:', acc_norm\n",
    "    save_confusion_matrix(y_test, yhat, TITLE+'_'+name+'_'+name_task+'_'+TITLE+'_'+class_weight+'_'+str(round(acc,5)), \n",
    "                      title=str(name)+'_'+name_task, \n",
    "                      info='Acc.: '+ str(round(acc*100,2))+',  Acc. norm.:'+str(round(acc_norm*100,2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN applying GridSearchCV. In param_grid, you can choose the parameters to do in GridSearch\n",
    "def KNeighbors_GridSearch(X, X_test, y, y_test, name_task):\n",
    "    name = 'KNeighbors'\n",
    "    print  name\n",
    "    #p=1 is manhattan distance, p=2 is euclidean_distance\n",
    "    param_grid = [{'n_neighbors': [3, 5, 8, 10, 15, 20], 'metric': ['euclidean', 'chebyshev', 'minkowski'], 'weights':['distance']}]\n",
    "\n",
    "    # KFold\n",
    "    nfolds = 3\n",
    "    kf = cross_validation.KFold(n=X.shape[0], n_folds=nfolds, shuffle=True, random_state=0)\n",
    "\n",
    "    yhat = np.empty_like(y)\n",
    "    acc = np.empty(nfolds)\n",
    "    acc_norm = np.empty(nfolds)\n",
    "    i = 0\n",
    "\n",
    "    best_acc = 0\n",
    "\n",
    "    for train_index, test_index in kf:\n",
    "        X_train, X_val = X[train_index], X[test_index]\n",
    "        y_train, y_val = y[train_index], y[test_index]\n",
    "\n",
    "        print \"Iteration KFold:\", i+1,'/',nfolds\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        # Standard parameters\n",
    "        clf = KNeighborsClassifier(n_jobs=1)\n",
    "\n",
    "        # We can change the scoring \"average_precision\", \"recall\", \"f1\"\n",
    "        clf = grid_search.GridSearchCV(clf, param_grid, scoring='f1_weighted', n_jobs=4, verbose=1)\n",
    "        clf.fit(X_train,y_train.ravel())\n",
    "\n",
    "        X_val = scaler.transform(X_val)\n",
    "        yhat[test_index] = clf.predict(X_val)\n",
    "\n",
    "        acc[i] = metrics.accuracy_score(yhat[test_index], y_val)\n",
    "        acc_norm[i] = normalized_accuracy(y_val, yhat[test_index])\n",
    "        #We save the parameters with the ones we obtain better accuract\n",
    "        if (acc_norm[i]>best_acc):\n",
    "            best_acc = acc_norm[i]\n",
    "            best_param = clf.best_params_\n",
    "            \n",
    "            \n",
    "        print \"\\t\" + str(clf.best_params_), acc[i]\n",
    "        print\n",
    "        i=i+1\n",
    "\n",
    "    print 'Mean accuracy: '+ str(np.mean(acc))\n",
    "      \n",
    "    # Once we have the best parameters for the training dataset, we obtain the accuraacy for the test dataset with the best parameters obtained. \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "    clf2 = KNeighborsClassifier(n_neighbors=best_param['n_neighbors'], \n",
    "                                               metric=best_param['metric'], weights=best_param['weights'])\n",
    "\n",
    "    clf2.fit(X_train, y)\n",
    "    yhat = clf2.predict(X_test)\n",
    "\n",
    "    acc = metrics.accuracy_score(yhat, y_test)\n",
    "    acc_norm = normalized_accuracy(y_test, yhat)\n",
    "    print best_param\n",
    "    print '\\tAccuracy:',acc, '\\t Accuracy normalized:', acc_norm\n",
    "    save_confusion_matrix(y_test, yhat, TITLE+'_'+name+'_'+name_task+'_'+TITLE+'_'+str(round(acc,5)), \n",
    "                      title=str(name)+'_'+name_task, \n",
    "                      info='Acc.: '+ str(round(acc,2))+',  Acc. norm.:'+str(round(acc_norm,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"A different dataset\n",
    "### Dataset Food related objects EGO\n",
    "###food_related_objects_EGO\n",
    "TITLE = 'Food_related_objects_EGO' \n",
    "DATASET_FOLDER = '../../data/datasets/food_related_objects_EGO'\n",
    "WEIGHTS_NAME = 'weights/weights_85-30_85-45_25_1040.h5'\n",
    "NUM_CLASSES = 2\n",
    "NUM_TRAIN_IMG = 2610 # just to check if the dataset was correctly loaded\n",
    "NUM_VAL_IMG = 1045   # just to check if the dataset was correctly loaded\n",
    "NUM_TEST_IMG = 1597   # just to check if the dataset was correctly loaded\n",
    "BATCH_SIZE = 32      # Depends on the available GPU memory\n",
    "MULTITASK=False\n",
    "##### GENERAL SETTINGS\n",
    "TRAIN_TXT = DATASET_FOLDER+'/train.txt'\n",
    "VAL_TXT = DATASET_FOLDER+'/val.txt'\n",
    "TEST_TXT=DATASET_FOLDER+'/test.txt'\n",
    "CLASSES = DATASET_FOLDER+'/classesID.txt'\n",
    "OUTPUTS = 'outputs/'\n",
    "WEIGHTS_PATH = OUTPUTS + WEIGHTS_NAME\n",
    "NAME_SAVED_MODEL = OUTPUTS + 'weights/weights_'\n",
    "CONF_MATRIX = OUTPUTS + 'confusion_matrices/cm_'\n",
    "ACCU_LOSS = OUTPUTS + 'accuracy_loss/acc_loss_'\n",
    "MODEL_INPUT_WIDTH = 299\n",
    "MODEL_INPUT_HEIGHT = 299\n",
    "SIZE = [MODEL_INPUT_WIDTH, MODEL_INPUT_HEIGHT]\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = get_data_model(True)\n",
    "X_train = np.vstack((X_train, X_val))\n",
    "y_train =  np.hstack((y_train, y_val))\n",
    "accuracy_classifiers(X_train, X_test, y_train, y_test, linear_svc=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FOR DOING EXPERIMENTS (small dataset)\n",
    "TITLE = 'multitask_test' \n",
    "DATASET_FOLDER = '../../data/datasets/Test_multitask'\n",
    "WEIGHTS_NAME = 'weights/weights_86-73_86-73_e99_i8799_multitask.h5'\n",
    "NUM_CLASSES = 7\n",
    "NUM_TRAIN_IMG = 100 # just to check if the dataset was correctly loaded\n",
    "NUM_VAL_IMG = 100   # just to check if the dataset was correctly loaded\n",
    "NUM_TEST_IMG = 100   # just to check if the dataset was correctly loaded\n",
    "BATCH_SIZE = 32      # Depends on the available GPU memory\n",
    "MULTITASK = True\n",
    "MULTITASK_LBL = [\"Food\", \"Table\", \"Social\"]\n",
    "\n",
    "##### GENERAL SETTINGS\n",
    "TRAIN_TXT = DATASET_FOLDER+'/train.txt'\n",
    "VAL_TXT = DATASET_FOLDER+'/val.txt'\n",
    "TEST_TXT=DATASET_FOLDER+'/test.txt'\n",
    "CLASSES = DATASET_FOLDER+'/classesID.txt'\n",
    "OUTPUTS = 'outputs/'\n",
    "WEIGHTS_PATH = OUTPUTS + WEIGHTS_NAME\n",
    "NAME_SAVED_MODEL = OUTPUTS + 'weights/weights_'\n",
    "CONF_MATRIX = OUTPUTS + 'confusion_matrices/cm_'\n",
    "ACCU_LOSS = OUTPUTS + 'accuracy_loss/acc_loss_'\n",
    "MODEL_INPUT_WIDTH = 299\n",
    "MODEL_INPUT_HEIGHT = 299\n",
    "SIZE = [MODEL_INPUT_WIDTH, MODEL_INPUT_HEIGHT]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run one of the datasets or insert anotherone. You can decide which one to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multitask Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All the dataset multitask\n",
    "TITLE = 'multitask_CNN_' \n",
    "DATASET_FOLDER = '../../data/datasets/multitask_dataset'\n",
    "WEIGHTS_NAME = 'weights/weights_70-19_72-67_42-58_e11_i5951_multitask_weighted_15.h5'\n",
    "NUM_CLASSES = 7\n",
    "NUM_TRAIN_IMG = 31708 # just to check if the dataset was correctly loaded\n",
    "NUM_VAL_IMG = 6795   # just to check if the dataset was correctly loaded\n",
    "NUM_TEST_IMG = 6794   # just to check if the dataset was correctly loaded\n",
    "\n",
    "BATCH_SIZE = 16      # Depends on the available GPU memory\n",
    "MULTITASK = True\n",
    "MULTITASK_LBL = [\"Food\", \"Table\", \"Social\"]\n",
    "\n",
    "##### GENERAL SETTINGS\n",
    "TRAIN_TXT = DATASET_FOLDER+'/train.txt'\n",
    "VAL_TXT = DATASET_FOLDER+'/val.txt'\n",
    "TEST_TXT=DATASET_FOLDER+'/test.txt'\n",
    "CLASSES = DATASET_FOLDER+'/classesID.txt'\n",
    "OUTPUTS = 'outputs/'\n",
    "WEIGHTS_PATH = OUTPUTS + WEIGHTS_NAME\n",
    "NAME_SAVED_MODEL = OUTPUTS + 'weights/weights_'\n",
    "CONF_MATRIX = OUTPUTS + 'confusion_matrices/cm_'\n",
    "ACCU_LOSS = OUTPUTS + 'accuracy_loss/acc_loss_'\n",
    "MODEL_INPUT_WIDTH = 299\n",
    "MODEL_INPUT_HEIGHT = 299\n",
    "SIZE = [MODEL_INPUT_WIDTH, MODEL_INPUT_HEIGHT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TITLE = 'full_dataset' \n",
    "DATASET_FOLDER =  '../../data/datasets/full_dataset'\n",
    "WEIGHTS_NAME = 'weights/weights_17-70_52-17_e5_i5945_full_dataset.h5'\n",
    "NUM_CLASSES = 12\n",
    "NUM_TRAIN_IMG = 31708 # just to check if the dataset was correctly loaded\n",
    "NUM_VAL_IMG = 6795   # just to check if the dataset was correctly loaded\n",
    "NUM_TEST_IMG = 6794   # just to check if the dataset was correctly loaded\n",
    "BATCH_SIZE = 16      # Depends on the available GPU memory\n",
    "MULTITASK = False\n",
    "\n",
    "##### GENERAL SETTINGS\n",
    "TRAIN_TXT = DATASET_FOLDER+'/train.txt'\n",
    "VAL_TXT = DATASET_FOLDER+'/val.txt'\n",
    "TEST_TXT=DATASET_FOLDER+'/test.txt'\n",
    "CLASSES = DATASET_FOLDER+'/classesID.txt'\n",
    "OUTPUTS = 'outputs/'\n",
    "WEIGHTS_PATH = OUTPUTS + WEIGHTS_NAME\n",
    "NAME_SAVED_MODEL = OUTPUTS + 'weights/weights_'\n",
    "CONF_MATRIX = OUTPUTS + 'confusion_matrices/cm_'\n",
    "ACCU_LOSS = OUTPUTS + 'accuracy_loss/acc_loss_'\n",
    "MODEL_INPUT_WIDTH = 299\n",
    "MODEL_INPUT_HEIGHT = 299\n",
    "SIZE = [MODEL_INPUT_WIDTH, MODEL_INPUT_HEIGHT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creating or loading the data (parameter load)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = get_data_model(load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.vstack((X_train, X_val))\n",
    "y_train =  np.vstack((y_train, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38503, 1024)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If the dataset is Full dataset, don't run this\n",
    "#Converting the binary matrix into a vector with the number of the class\n",
    "y_train_food = to_one_vector(y_train, [0,3])\n",
    "y_test_food = to_one_vector(y_test, [0,3])\n",
    "\n",
    "y_train_social =  to_one_vector(y_train, [3,5])\n",
    "y_test_social =  to_one_vector(y_test, [3,5])\n",
    "\n",
    "y_train_table =  to_one_vector(y_train, [5,7])\n",
    "y_test_table =  to_one_vector(y_test, [5,7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC\n",
      "balanced\n",
      "Iteration KFold: 1 / 5\n",
      "Fitting 3 folds for each of 14 candidates, totalling 42 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 out of  42 | elapsed: 23.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t{'loss': 'hinge', 'C': 10, 'class_weight': 'balanced'} 0.933125568108\n",
      "\n",
      "Iteration KFold: 2 / 5\n",
      "Fitting 3 folds for each of 14 candidates, totalling 42 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 out of  42 | elapsed: 24.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t{'loss': 'squared_hinge', 'C': 10, 'class_weight': 'balanced'} 0.942215296715\n",
      "\n",
      "Iteration KFold: 3 / 5\n",
      "Fitting 3 folds for each of 14 candidates, totalling 42 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 out of  42 | elapsed: 24.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t{'loss': 'squared_hinge', 'C': 1, 'class_weight': 'balanced'} 0.932346448513\n",
      "\n",
      "Iteration KFold: 4 / 5\n",
      "Fitting 3 folds for each of 14 candidates, totalling 42 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 out of  42 | elapsed: 24.3min finished\n"
     ]
    }
   ],
   "source": [
    "#Grid search for food (dataset multitask) using linear SVM\n",
    "accuracy_classifiers(X_train, X_test, y_train_food, y_test_food, linear_svc=True, name_task='CNN_Food', class_weight='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighbors\n",
      "Iteration KFold: 1 / 5\n",
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 16.3min\n",
      "[Parallel(n_jobs=4)]: Done  96 out of  96 | elapsed: 31.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t{'n_neighbors': 10, 'metric': 'euclidean', 'weights': 'distance'} 0.708868978055\n",
      "\n",
      "Iteration KFold: 2 / 5\n",
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 25.2min\n",
      "[Parallel(n_jobs=4)]: Done  96 out of  96 | elapsed: 45.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t{'n_neighbors': 10, 'metric': 'chebyshev', 'weights': 'distance'} 0.700298662511\n",
      "\n",
      "Iteration KFold: 3 / 5\n",
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 24.7min\n",
      "[Parallel(n_jobs=4)]: Done  96 out of  96 | elapsed: 43.2min finished\n"
     ]
    }
   ],
   "source": [
    "#KNN with Grid Search to find the best parameters for multitask social\n",
    "accuracy_classifiers(X_train, X_test, y_train_social, y_test_social, k_neighbors=True, name_task='CNN_Social')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoosting\n",
      "Iteration KFold: 1 / 3\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=4)]: Done  36 out of  36 | elapsed: 53.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t{'n_estimators': 100, 'learning_rate': 0.1, 'min_samples_leaf': 5} 0.943202181535\n",
      "\n",
      "Iteration KFold: 2 / 3\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=4)]: Done  36 out of  36 | elapsed: 67.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t{'n_estimators': 100, 'learning_rate': 0.1, 'min_samples_leaf': 5} 0.941639395356\n",
      "\n",
      "Iteration KFold: 3 / 3\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-878a18dd233f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccuracy_classifiers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_food\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_food\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_boosting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_task\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'CNN_Food'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-baa5d5f88126>\u001b[0m in \u001b[0;36maccuracy_classifiers\u001b[0;34m(X_train, X_test, y_train, y_test, not_grid_search, adaboost, gradient_boosting, random_forest, svc, linear_svc, k_neighbors, name_task, class_weight)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mAdaboost_GridSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_task\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname_task\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgradient_boosting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mGradientBoosting_GridSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_task\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname_task\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_forest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mRandomForest_GridSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_task\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname_task\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-043528f9cb90>\u001b[0m in \u001b[0;36mGradientBoosting_GridSearch\u001b[0;34m(X, X_test, y, y_test, name_task)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# We can change the scoring \"average_precision\", \"recall\", \"f1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1_weighted'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mX_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m         \"\"\"\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    571\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m                                     error_score=self.error_score)\n\u001b[0;32m--> 573\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m                 for train, test in cv)\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "accuracy_classifiers(X_train, X_test, y_train_food, y_test_food, gradient_boosting=True, name_task='CNN_Food')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary with the best parameters of the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_SVM_predict(X, X_test, y, y_test, name_task, class_weight):\n",
    "    name='linear_SVM'\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "    clf2 = svm.LinearSVC(loss='squared_hinge', C=0.01, class_weight='balanced')\n",
    "\n",
    "    clf2.fit(X_train, y)\n",
    "    yhat = clf2.predict(X_test)\n",
    "\n",
    "    acc = metrics.accuracy_score(yhat, y_test)\n",
    "    acc_norm = normalized_accuracy(y_test, yhat)\n",
    "    title=TITLE+'_'+name+'_'+name_task+'_'+TITLE+'_'+class_weight+'_'+str(round(acc,5))\n",
    "    np.save(title+\"_real\", y_test)\n",
    "    np.save(title+\"_predicted\", yhat)\n",
    "    print '\\tAccuracy:',acc, '\\t Accuracy normalized:', acc_norm\n",
    "    save_confusion_matrix(y_test, yhat, title, \n",
    "                      title=str(name)+'_'+name_task, \n",
    "                      info='Acc.: '+ str(round(acc*100,2))+',  Acc. norm.:'+str(round(acc_norm*100,2)))\n",
    "\n",
    "def knn_predict(X, X_test, y, y_test, name_task, class_weight):\n",
    "    name='knn'\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "    clf2 = KNeighborsClassifier(n_neighbors=5, metric='euclidean', weights='distance')\n",
    "    clf2.fit(X_train, y)\n",
    "    yhat = clf2.predict(X_test)\n",
    "\n",
    "    acc = metrics.accuracy_score(yhat, y_test)\n",
    "    acc_norm = normalized_accuracy(y_test, yhat)\n",
    "    title=TITLE+'_'+name+'_'+name_task+'_'+TITLE+'_'+class_weight+'_'+str(round(acc,5))\n",
    "    np.save(title+\"_real\", y_test)\n",
    "    np.save(title+\"_predicted\", yhat)\n",
    "    print '\\tAccuracy:',acc, '\\t Accuracy normalized:', acc_norm\n",
    "    save_confusion_matrix(y_test, yhat, title, \n",
    "                      title=str(name)+'_'+name_task, \n",
    "                      info='Acc.: '+ str(round(acc*100,2))+',  Acc. norm.:'+str(round(acc_norm*100,2)))\n",
    "\n",
    "def gradient_predict(X, X_test, y, y_test, name_task, class_weight):\n",
    "    name='gradient_boosting'\n",
    "    # Once we have the best parameters for the training dataset, we obtain the accuraacy for the test dataset with the best parameters obtained. \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "    clf2 = ensemble.GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, \n",
    "                                              min_samples_leaf=5)\n",
    "       \n",
    "    clf2.fit(X_train, y)\n",
    "    yhat = clf2.predict(X_test)\n",
    "\n",
    "    acc = metrics.accuracy_score(yhat, y_test)\n",
    "    acc_norm = normalized_accuracy(y_test, yhat)\n",
    "    title=TITLE+'_'+name+'_'+name_task+'_'+TITLE+'_'+class_weight+'_'+str(round(acc,5))\n",
    "    np.save(title+\"_real\", y_test)\n",
    "    np.save(title+\"_predicted\", yhat)\n",
    "    print '\\tAccuracy:',acc, '\\t Accuracy normalized:', acc_norm\n",
    "    save_confusion_matrix(y_test, yhat, title, \n",
    "                      title=str(name)+'_'+name_task, \n",
    "                      info='Acc.: '+ str(round(acc*100,2))+',  Acc. norm.:'+str(round(acc_norm*100,2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multitask Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.940682955549 \t Accuracy normalized: 0.510606518063\n",
      "\tPrecision: 0.922883108749 \t Recall: 0.940682955549\n",
      "\tF1 score: 0.926955460564\n"
     ]
    }
   ],
   "source": [
    "#n_neighbors 10, metric euclidean, weights distance\n",
    "knn_predict(X_train, X_test, y_train_food, y_test_food, name_task='CNN_food', class_weight='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.940094200765 \t Accuracy normalized: 0.604797363141\n",
      "\tPrecision: 0.932518476166 \t Recall: 0.940094200765\n",
      "\tF1 score: 0.935845050466\n"
     ]
    }
   ],
   "source": [
    "linear_SVM_predict(X_train, X_test, y_train_food, y_test_food, name_task='CNN_Food', class_weight='balanced')\n",
    "#loss:hinge, C=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.942302031204 \t Accuracy normalized: 0.486195608494\n",
      "\tPrecision: 0.930131735077 \t Recall: 0.942302031204\n",
      "\tF1 score: 0.924913528735\n"
     ]
    }
   ],
   "source": [
    "#n_estimators=100, learningrate=0.1, min samplesleaf=5\n",
    "gradient_predict(X_train, X_test, y_train_food, y_test_food, name_task='CNN_food', class_weight='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Social"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.706358551663 \t Accuracy normalized: 0.673813299174\n",
      "\tPrecision: 0.700187136289 \t Recall: 0.706358551663\n",
      "\tF1 score: 0.700490703491\n"
     ]
    }
   ],
   "source": [
    "#n_neighbors 15, metric euclidean, weights distance\n",
    "knn_predict(X_train, X_test, y_train_social, y_test_social, name_task='CNN_social', class_weight='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.74065351781 \t Accuracy normalized: 0.731517249839\n",
      "\tPrecision: 0.744341248176 \t Recall: 0.74065351781\n",
      "\tF1 score: 0.742053358226\n"
     ]
    }
   ],
   "source": [
    "linear_SVM_predict(X_train, X_test, y_train_social, y_test_social, name_task='CNN_social', class_weight='balanced')\n",
    "#loss: hinge, C=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.700471003827 \t Accuracy normalized: 0.657010861582\n",
      "\tPrecision: 0.692999482391 \t Recall: 0.700471003827\n",
      "\tF1 score: 0.688554313675\n"
     ]
    }
   ],
   "source": [
    "#n_estimators=100, learningrate=0.1, min samplesleaf=5\n",
    "gradient_predict(X_train, X_test, y_train_social, y_test_social, name_task='CNN_social', class_weight='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.795849278775 \t Accuracy normalized: 0.743611947142\n",
      "\tPrecision: 0.792085107276 \t Recall: 0.795849278775\n",
      "\tF1 score: 0.793587797202\n"
     ]
    }
   ],
   "source": [
    "#n_neighbors 10, metric euclidean, weights distance\n",
    "knn_predict(X_train, X_test, y_train_table, y_test_table, name_task='CNN_table', class_weight='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.819546658817 \t Accuracy normalized: 0.821890316539\n",
      "\tPrecision: 0.840727062236 \t Recall: 0.819546658817\n",
      "\tF1 score: 0.824909222874\n"
     ]
    }
   ],
   "source": [
    "linear_SVM_predict(X_train, X_test, y_train_table, y_test_table, name_task='CNN_table', class_weight='balanced')\n",
    "#loss= hinge, C=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy: 0.794524580512 \t Accuracy normalized: 0.7091399054\n",
      "\tPrecision: 0.785617395649 \t Recall: 0.794524580512\n",
      "\tF1 score: 0.782646237233\n"
     ]
    }
   ],
   "source": [
    "#n_estimators=100, learningrate=0.1, min samplesleaf=5\n",
    "gradient_predict(X_train, X_test, y_train_table, y_test_table, name_task='CNN_table', class_weight='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = get_data_model(load=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38503,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.vstack((X_train, X_val))\n",
    "y_train =  np.hstack((y_train, y_val))\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#n_estimators=100, learningrate=0.1, min samplesleaf=5\n",
    "gradient_predict(X_train, X_test, y_train, y_test, name_task='CNN', class_weight='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#n_neighbors 5, metric euclidean, weights distance\n",
    "knn_predict(X_train, X_test, y_train, y_test, name_task='CNN', class_weight='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_SVM_predict(X_train, X_test, y_train, y_test, name_task='CNN', class_weight='balanced')\n",
    "#{'loss': 'squared_hinge', 'C': 0.01, 'class_weight': ''}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
